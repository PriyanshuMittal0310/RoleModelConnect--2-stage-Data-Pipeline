# -*- coding: utf-8 -*-
"""Raw_Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U2rTsQHY-rkWoYgqYiuOmg941Cl3f2Gh
"""

# Import required libraries
import requests
from bs4 import BeautifulSoup
import os
import time
import argparse
import urllib.robotparser as robotparser
from urllib.parse import urlparse
import re

# Configuration
OUTPUT_FOLDER = "Raw_Data"
REQUEST_DELAY = 2  # seconds between requests to avoid overloading servers

# Create output folder if it doesn't exist
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

def clean_filename(text):
    """Clean text to create valid filename"""
    # Remove invalid filename characters
    text = re.sub(r'[<>:"/\\|?*]', '', text)
    # Replace spaces with underscores
    text = text.replace(' ', '_')
    # Limit length
    return text[:50]

USER_AGENT = "RoleModelConnectBot/1.0 (+https://example.com)"

def is_allowed_by_robots(url, user_agent=USER_AGENT):
    """
    Check robots.txt to see if we are allowed to scrape this URL.
    Returns True if allowed or robots.txt is unreachable, False if explicitly disallowed.
    """
    parsed = urlparse(url)
    robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
    rp = robotparser.RobotFileParser()
    rp.set_url(robots_url)

    try:
        rp.read()
    except Exception:
        print(f"  ⚠ Could not read robots.txt from {robots_url}. Proceeding cautiously.")
        # If robots.txt can’t be read, we choose to proceed but log it.
        return True

    allowed = rp.can_fetch(user_agent, url)
    if not allowed:
        print(f"  ✗ Disallowed by robots.txt for {parsed.netloc}. Skipping this URL.")
    return allowed

def scrape_url(url, headers=None):
    """
    Scrape text content from a single URL

    Args:
        url (str): The URL to scrape
        headers (dict): Optional headers for the request

    Returns:
        str: Extracted text content or None if failed
    """
    if headers is None:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    try:
        print(f"  Fetching: {url}")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()

        # Parse HTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # Remove script and style elements
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()

        # Try to find main content area (common patterns)
        main_content = None

        # Try different content selectors
        selectors = [
            'article',
            'main',
            '[role="main"]',
            '.article-content',
            '.post-content',
            '.entry-content',
            '#content',
            '.content'
        ]

        for selector in selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break

        # If no main content found, use body
        if not main_content:
            main_content = soup.body

        # Extract text
        if main_content:
            # Get all paragraphs
            paragraphs = main_content.find_all(['p', 'h1', 'h2', 'h3', 'blockquote'])
            text = '\n\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        else:
            text = soup.get_text(separator='\n\n', strip=True)

        # Clean up excessive whitespace
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r' {2,}', ' ', text)

        print(f"  ✓ Successfully scraped {len(text)} characters")
        return text

    except requests.exceptions.RequestException as e:
        print(f"  ✗ Error scraping {url}: {str(e)}")
        return None
    except Exception as e:
        print(f"  ✗ Unexpected error: {str(e)}")
        return None

def save_raw_data(role_model_name, source_index, url, text_content):
    """
    Save scraped text to /Raw_Data/ folder

    Args:
        role_model_name (str): Name of the role model
        source_index (int): Index of the source (1, 2, 3, etc.)
        url (str): Original URL (for reference)
        text_content (str): The scraped text content

    Returns:
        str: Path to saved file
    """
    # Create clean filename
    clean_name = clean_filename(role_model_name)

    # Extract domain name from URL for more descriptive filename
    domain = urlparse(url).netloc.replace('www.', '').split('.')[0]
    domain_clean = clean_filename(domain)

    filename = f"{clean_name}_{domain_clean}_source_{source_index}.txt"
    filepath = os.path.join(OUTPUT_FOLDER, filename)

    # Save to file with UTF-8 encoding
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"Source URL: {url}\n")
        f.write(f"Scraped Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Role Model: {role_model_name}\n")
        f.write("="*80 + "\n\n")
        f.write(text_content)

    print(f"  ✓ Saved to: {filepath}")
    return filepath

def collect_data_for_role_model(role_model_name, url_list):
    """
    Main function to scrape multiple URLs for one role model

    Args:
        role_model_name (str): Name of the role model
        url_list (list): List of URLs to scrape

    Returns:
        list: List of successfully saved file paths
    """
    print(f"\n{'='*80}")
    print(f"Collecting data for: {role_model_name}")
    print(f"{'='*80}")

    saved_files = []

    for index, url in enumerate(url_list, 1):
        print(f"\nSource {index}/{len(url_list)}:")

        # Check robots.txt before scraping
        if not is_allowed_by_robots(url):
            print("  Skipping due to robots.txt rules.")
            continue

        # Scrape the URL
        text = scrape_url(url)

        if text and len(text) > 500:  # Minimum content check
            filepath = save_raw_data(role_model_name, index, url, text)
            saved_files.append(filepath)
        else:
            print(f"  ⚠ Skipped: Insufficient content (less than 500 characters)")

        # Delay between requests to be respectful
        if index < len(url_list):
            print(f"  Waiting {REQUEST_DELAY} seconds before next request...")
            time.sleep(REQUEST_DELAY)

    print(f"\n✓ Completed: {len(saved_files)}/{len(url_list)} sources successfully scraped")
    return saved_files

def parse_args():
    """
    Parse optional CLI arguments.

    Example:
      python raw_data.py --name "Selena Gomez" --urls URL1 URL2 URL3
    """
    parser = argparse.ArgumentParser(
        description="RoleModelConnect - Phase 1: Data Collection Pipeline"
    )
    parser.add_argument(
        "--name",
        type=str,
        help="Role model name (e.g., 'Selena Gomez')"
    )
    parser.add_argument(
        "--urls",
        nargs="+",
        help="One or more seed URLs for this role model"
    )
    return parser.parse_args()


def interactive_input():
    """Fallback interactive input for role model name and URLs."""
    print("\nInteractive mode: provide role model name and URLs.")
    role_model_name = input("Enter Role Model Name (e.g., 'Selena Gomez'): ").strip()
    while not role_model_name:
        role_model_name = input("Please enter a valid name: ").strip()

    while True:
        try:
            n = int(input("How many URLs do you want to scrape? (e.g., 2 or 3): "))
            if n > 0:
                break
            print("Please enter a positive number.")
        except ValueError:
            print("Please enter a valid integer.")

    urls = []
    for i in range(1, n + 1):
        url = input(f"Enter URL #{i}: ").strip()
        while not url:
            url = input(f"URL #{i} cannot be empty. Re-enter: ").strip()
        urls.append(url)

    return role_model_name, urls

def main():
    """Main execution function"""
    print("=" * 80)
    print("RoleModelConnect - Phase 1: Data Collection Pipeline")
    print("=" * 80)
    print(f"Output folder: {OUTPUT_FOLDER}")
    print("=" * 80)

    args = parse_args()

    # --- CLI MODE: only one role model per run (explicit arguments) ---
    if args.name and args.urls:
        role_model_name = args.name.strip()
        url_list = args.urls
        print("\nRunning in CLI mode (arguments provided).")

        all_saved_files = collect_data_for_role_model(role_model_name, url_list)

        print("\n" + "=" * 80)
        print("COLLECTION COMPLETE (CLI MODE)")
        print("=" * 80)
        print(f"Total files saved: {len(all_saved_files)}")
        for filepath in all_saved_files:
            print(f"  - {filepath}")
        return  # end program here

    # --- INTERACTIVE MODE: multiple role models in one run ---
    print("\nNo CLI arguments detected. Switching to interactive mode.")

    all_saved_files_global = []

    while True:
        # 1. Get one role model + its URLs
        role_model_name, url_list = interactive_input()

        # 2. Collect data for this role model
        saved_files = collect_data_for_role_model(role_model_name, url_list)
        all_saved_files_global.extend(saved_files)

        # 3. Ask if user wants to add another role model
        choice = input(
            "\nDo you want to scrape data for another role model? (y/n): "
        ).strip().lower()

        if choice != "y":
            break

    # --- Final summary for interactive session ---
    print("\n" + "=" * 80)
    print("COLLECTION COMPLETE (INTERACTIVE MODE)")
    print("=" * 80)
    print(f"Total files saved in this session: {len(all_saved_files_global)}")

    if all_saved_files_global:
        print("\nSaved files:")
        for filepath in all_saved_files_global:
            print(f"  - {filepath}")

    print(f"\n✓ All raw data saved to '{OUTPUT_FOLDER}/' folder")
    print("\nNext steps:")
    print("1. Review the scraped text files")
    print("2. Proceed to Phase 2: Data Curation")
    print("3. Create JSON entries based on these raw text files")

if __name__ == "__main__":
    main()